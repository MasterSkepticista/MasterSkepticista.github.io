---
title: 'The Blessing of Higher Dimensions'
date: 2024-11-11
permalink: /posts/2024/11/jl-lemma/
excerpt: "Because we have heard enough about the curse of dimensionality."
tags:
  - cs
toc: true
---

Grant recently published a [video](https://www.youtube.com/watch?v=piJkuavhV50) on how certain geometry puzzles become trivial to solve, when one additional dimension is available for the kind of geometry at hand. Math provides the necessary abstraction to think (and compute) in a dimensionality that far exceeds human imagination of 3 dimensions.

Modern foundation models use more than 10,000 feature vector dimensions. As of writing, no tools exist to understand what exactly happens in each of these dimensions. The goal of this post is to help you reason through the working of neural network dimensionality, and _why_ more dimensions typically yield better models. If you have read Chris Olah's decade-old post on [manifold hypothesis](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/), first half of this post should be a cakewalk.

<!-- Footnote: Training data size and quality has to keep up of course. -->

<!-- The rest of this blog  context on _why_ high dimensionality is a blessing for deep neural networks, all things considered. -->

## Data as a Manifold

Classification is a common deep learning task. Even foundation LLMs are trained on next-token prediction, which is a classification task. We can view classification as a way of transforming the data manifold into representations that can be sliced into individual classes.

> The manifold hypothesis is that natural data forms **lower-dimensional manifolds** in its embedding space. [...] If you believe this, then **the task of a classification algorithm is fundamentally to separate a bunch of tangled manifolds.**

Neural networks (NNs) are efficient at extracting representations from data by augmenting it into certain size of representations. Dimensionality of the representations that NNs extract from natural data depends on what is expected of them. For example, sentiment classification models may require a smaller data manifold, than say, summarizing it, if both models are trained from scratch until convergence.

```python
import optax
import flax.linen as nn
import numpy as np
import jax, jax.numpy as jnp
from sklearn import datasets

import plotly.express as px
import plotly.graph_objects as go

rng = jax.random.PRNGKey(42)
```

## Classification in 2D üêú
Consider this binary classification problem for an ant living in 2D space. The inner blue blob is class `1`, and the red ring is class `0`.

```python
x, y = datasets.make_circles(n_samples=2048, noise=0.15, factor=0.1)
x = (x - x.mean(axis=0)) / x.std(axis=0)
fig = px.scatter(x=x[:, 0], y=x[:, 1], color=y, color_continuous_scale='RdBu', opacity=0.5)
fig.update_layout(width=600, height=600, coloraxis_showscale=False, template="plotly_white")
fig.show()
```
<img style="display: block; margin: auto; width: 100%;" src="/images/posts/jl_lemma/circles.png">

If we consider this ant's brain as a classifier, we can construct a neural network to emulate it. We know via the [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), that a neural network with at least one hidden layer of arbitrary width can approximate **any** function.

```python
class Brain(nn.Module):
  """An Ant Brain with a single hidden layer."""
  dimensions: int

  @nn.compact
  def __call__(self, inputs):
    out = {}
    x = nn.Dense(self.dimensions, kernel_init=nn.initializers.xavier_uniform())(inputs)
    x = out["activation"] = nn.tanh(x)
    x = nn.Dense(1, name="classifier")(x)
    return x, out
```

We need a some boilerplate to train this ant brain. This is a vanilla SGD-based training loop with no fancy regularization or momentum.

```python
def fit(brain: nn.Module, params, x: jnp.ndarray, y: jnp.ndarray):
  """Fit a classification model to the given data."""
  
  @jax.jit
  def train_step(params, x, y, opt_state):
    def loss_fn(params):
      logits, out = brain.apply({"params": params}, x)
      logits = jnp.squeeze(logits)
      loss = optax.sigmoid_binary_cross_entropy(logits, y).mean()
      acc = jnp.mean((logits > 0.5) == y)
      metrics = {"loss": loss, "acc": acc}
      return loss, (metrics, out["activation"])

    (_, (metrics, acts)), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)
    updates, opt_state = tx.update(grads, opt_state, params)
    params = optax.apply_updates(params, updates)

    return params, opt_state, metrics, acts
  
  # Initialize optimizer
  total_steps = 4_000
  tx = optax.sgd(learning_rate=0.01)
  opt_state = tx.init(params)

  # Training loop
  step = 1
  for step in range(1, total_steps + 1):
    params, opt_state, metrics, acts = train_step(params, x, y, opt_state)
    if step % 200 == 0 or step == 1:
      print(f'Step {step}, Loss: {metrics["loss"]:.4f}, Acc: {metrics["acc"]:.4f}')

  return params
```
To match the ant's 2D world, we will initialize our ant brain with two dimensions. Visualizing the activation manifold of this tiny brain shows the learnt decision boundary. Since activation manifold is 2D, our decision boundary will be a line.

```python
brain = Brain(dimensions=2)
rng, rng_init = jax.random.split(rng)
params = brain.init(rng_init, jnp.zeros_like(x))["params"]
params = fit(brain, params, x, y)
```
<img style="display: block; margin: auto; width: 100%;" src="/images/posts/jl_lemma/manifold_2d.gif">

The entire `blue` cluster is cornered with `red` ring stretched out along perpendicular directions. It is mathematically impossible to achieve ~100% accuracy on this problem unless we have more dimensions of space. Why? Because the outer ring fully covers the inner one. No line segment can partition these two clusters in 2D space. The fact that despite topological limitations, this 2D ant-brain crossed ~85% accuracy on this dataset tells us how far even this 2-braincell network can twist the data manifold for classification.

## Adding the third-dimension

Let's see what happens when this ant brain is given one extra neuron - to think in 3 dimensions. Note that our activation manifold will now be 3D, too.

```python
brain = Brain(dimensions=3)
rng, rng_init = jax.random.split(rng)
params = brain.init(rng_init, jnp.zeros_like(x))["params"]
params = fit(brain, params, x, y)
```

<iframe src="/images/posts/jl_lemma/manifold_3d.html" width="100%" height="420px" frameBorder="0"></iframe>

The decision boundary is now a plane, which can separate the two classes with ~100% accuracy. This is because the network can now stretch the center cluster out across z-axis, and slice a plane orthogonal to it.

<!-- 
The optimal solution under this constraint, is to draw a boundary on one of the empty spaces between the classes (marked as green stars). Any other decision boundary leads to worse accuracy.

> "OK, give me a break, when was the last time someone used single layer networks?"

Fair, let's add moar layers. This time, our ant brain has 8 hidden layers with one node each.

```python
layers = []
for _ in range(8):
  layers.extend([nn.Dense(1), nn.tanh])
layers.append(nn.Dense(1))

brain = nn.Sequential(layers)
rng, rng_init = jax.random.split(rng)
params = brain.init(rng_init, jnp.ones((1, 1)))["params"]

fit(params, x, y, num_epochs=1000)
# Epoch 1, Loss: 0.7012, Acc: 0.6667
# Epoch 500, Loss: 0.6782, Acc: 0.6667
# Epoch 1000, Loss: 0.6604, Acc: 0.6667
```

There is an important lesson here. If the data manifold cannot be twisted even using multiple applications of non-linearities, one needs additional dimensions of space to achieve the same. For our ant brain, this means gaining a superpower to see not one, but **two** dimensions.

```python
brain = nn.Sequential([
  nn.Dense(2), nn.tanh, nn.Dense(1)
])
rng, rng_init = jax.random.split(rng)
params = brain.init(rng_init, jnp.ones((1, 1)))["params"]

fit(params, x, y, num_epochs=1000)
# Epoch 1, Loss: 0.7222, Acc: 0.3333
# Epoch 500, Loss: 0.0425, Acc: 1.0000
# Epoch 1000, Loss: 0.0126, Acc: 1.0000
```

Let's visualize what just happened here.

## 2D to 3D Expansion (Optional)

## Choosing how many dimensions
We cannot determine the ideal number of dimensions required to disentangle a data manifold represented by trillions of tokens. The reason we employ neural networks, are because we want _them_ to untangle the manifold and approximate in as many dimensions we can afford with our hardware budget [footnote].

Orthogonality, max capacity

Hardware limits

Can we afford approximate orthogonality? What do we gain?

## JL Lemma
Now we can afford exponentially more (approximately) orthogonal dimensions.

## What it means for future models?

1. Higher the dimensionality, higher the odds of two random vectors being orthogonal
2. If one can tolerate approximate orthogonality, one can pack many more (nearly) orthogonal vectors in a small space.
3. 
<img style="display: block; margin: auto; width: 50%;" src="/images/posts/jl_lemma/scatter1d.png"> -->